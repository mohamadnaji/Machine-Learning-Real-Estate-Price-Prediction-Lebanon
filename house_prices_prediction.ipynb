{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohamadnaji/Machine-Learning-Real-Estate-Price-Prediction-Lebanon/blob/features%2Ftrying-to-enhance/house_prices_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v5mm4amQRrm"
      },
      "source": [
        "# House Prices Prediction using TensorFlow Decision Forests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVOXAyXl3-fA"
      },
      "source": [
        "## Import the library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T23bn9d5mcrP",
        "outputId": "f638dd22-286f-4d07-8bf0-5c93f6d19ee2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "IGmyjJJatzBZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Comment this if the data visualisations doesn't work on your side\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3vxMmCPvqpf"
      },
      "source": [
        "## Load the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVMPH_IDOBH2",
        "outputId": "87e67f86-39bb-4cde-98ab-a7e9719a2c57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full train dataset shape is (10003, 50)\n"
          ]
        }
      ],
      "source": [
        "# train_file_path = \"/content/drive/Othercomputers/MyMonty Laptop/university M2/M2 final project/Dataset/step 3 - with spatial analysis/SELL_REAL_ESTATE_2024-09-15-2.xlsx\"\n",
        "train_file_path = \"/content/drive/Othercomputers/MyMonty Laptop/university M2/M2 final project/Dataset/step 3 - with spatial analysis/SELL_REAL_ESTATE_2024-11-17.xlsx\"\n",
        "# train_file_path = \"/content/drive/MyDrive/university M2/M2 final project/Dataset/step 3 - with spatial analysis/SELL_REAL_ESTATE_2024-10-29 - Copy.xlsx\"\n",
        "# Load the Excel file into a DataFrame\n",
        "data = pd.read_excel(train_file_path)\n",
        "\n",
        "print(\"Full train dataset shape is {}\".format(data.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsBvNK9UMgMG"
      },
      "source": [
        "## preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "N93vwLGS-w1h"
      },
      "outputs": [],
      "source": [
        "# Function to extract numbers\n",
        "def extract_number(value):\n",
        "    # Remove any non-numeric characters\n",
        "    number = re.sub(r\"[^\\d]\", \"\", value)\n",
        "    return int(number) if number else None\n",
        "\n",
        "# Apply the function to the 'Currency' column\n",
        "data['Price'] = data['price'].apply(extract_number)\n",
        "data = data.drop(columns=['price' ,'Listing Type', 'Join_Count', 'creation date',\n",
        "                          'NEAR_FID', 'OBJECTID', 'TARGET_FID', 'source',\n",
        "                          'source_name', 'Reference Id', 'img_src', 'description'\n",
        "                          ,'NEAR_UNIVERSITY_DIST'], inplace=False)\n",
        "\n",
        "data = data.drop(columns=[ 'X', 'Y'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "g3xYcQWKbxOU"
      },
      "outputs": [],
      "source": [
        "data = data.dropna(subset=['Price'])\n",
        "\n",
        "# Assuming your DataFrame is named 'data' and the columns are 'Type' and 'Size'\n",
        "data = data[data['Property Type'] == 'Apartment']\n",
        "data = data[data['Floors'].isna()]\n",
        "data = data[data['Bedrooms'].notna()]\n",
        "data = data[data['Bathrooms'].notna()]\n",
        "data = data[data['Size (mÂ²)'] < 800]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "GOrmGXiHomlu"
      },
      "outputs": [],
      "source": [
        "# Count the number of rows for each governorate\n",
        "governorate_counts = data['governorate'].value_counts()\n",
        "\n",
        "# Get the top 3 governorates with the highest row counts\n",
        "top_3_governorates = governorate_counts.head(3).index\n",
        "\n",
        "# Filter the data for only the top 3 governorates\n",
        "data = data[data['governorate'].isin(top_3_governorates)]\n",
        "\n",
        "data.loc[data['governorate'] == 'Keserwan-Jbeil', 'governorate'] = 'Mont-Liban'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfFLmAiakhqH",
        "outputId": "6f0d9003-da1b-40ae-8790-e0da21f1b556"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7204, 36)\n"
          ]
        }
      ],
      "source": [
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrbDLAwrM2Ml"
      },
      "source": [
        "Dropping the column that contains more than 1000 null value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFlI3_ToxDII",
        "outputId": "4ce0b773-f4ef-4b6c-d9fc-089c1bafc830"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7204, 29)"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ],
      "source": [
        "# Filter columns based on null value counts\n",
        "columns_to_drop = data.columns[data.isnull().sum() > 1500]\n",
        "\n",
        "# Drop columns with null value counts exceeding the threshold\n",
        "data = data.drop(columns=columns_to_drop)\n",
        "\n",
        "# # Remaining columns\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA_v408l416n"
      },
      "source": [
        "### Use data augmentation method to add more data for high prices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8ETa-NKdTNC",
        "outputId": "39d8839f-4d5a-4ae9-ab0e-a21a7b60ea69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of new samples 794\n",
            "Augmented data shape: (7769, 29)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Identify numerical, binary, and categorical features\n",
        "numerical_features = data.select_dtypes(include=[np.number]).columns\n",
        "binary_features = [col for col in numerical_features if data[col].nunique() == 2]\n",
        "continuous_features = [col for col in numerical_features if col not in binary_features]\n",
        "categorical_features = data.select_dtypes(exclude=[np.number]).columns\n",
        "\n",
        "# Filter high-priced data (e.g., Price >= 500k) 600 1.2\n",
        "high_price_data = data[data['Price'] >= 700000]\n",
        "\n",
        "# Number of synthetic samples to generate (e.g., 50% of high-price data)\n",
        "n_samples = int(0.8 * len(high_price_data))\n",
        "print(f'Number of new samples {n_samples}')\n",
        "\n",
        "# Generate synthetic samples with Gaussian noise for continuous features only\n",
        "synthetic_samples = []\n",
        "for _ in range(n_samples):\n",
        "    sample = high_price_data.sample(n=1).copy()\n",
        "\n",
        "    # Add Gaussian noise to continuous features SCALE=0.05\n",
        "    noise = np.random.normal(loc=0, scale=0.05, size=len(continuous_features))\n",
        "    sample[continuous_features] += sample[continuous_features] * noise\n",
        "\n",
        "    # # Ensure X and Y columns are minimally altered (0.001 or less)\n",
        "    # if 'X' in continuous_features and 'Y' in continuous_features:\n",
        "    #     xy_noise = np.random.normal(loc=0, scale=0.001, size=2)  # Generate noise for X and Y\n",
        "    #     sample['X'] += xy_noise[0]\n",
        "    #     sample['Y'] += xy_noise[1]\n",
        "\n",
        "    # Ensure binary features remain unchanged\n",
        "    sample[binary_features] = sample[binary_features].round().clip(0, 1)\n",
        "\n",
        "    # Append the augmented sample\n",
        "    synthetic_samples.append(sample)\n",
        "\n",
        "# Create a DataFrame from synthetic samples\n",
        "augmented_high_price_data = pd.concat(synthetic_samples, ignore_index=True)\n",
        "\n",
        "# Concatenate the augmented data with the original data\n",
        "data = pd.concat([data, augmented_high_price_data], ignore_index=True)\n",
        "\n",
        "data = data[data['Price'] < 2000000]\n",
        "data = data[data['Price'] > 15000]\n",
        "\n",
        "integer_features = ['Bedrooms', 'Bathrooms']\n",
        "data[integer_features] = np.ceil(data[integer_features]).astype(int)\n",
        "\n",
        "print(\"Augmented data shape:\", data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6tbJCsrNFWb"
      },
      "source": [
        "Encoding the object features using one label encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "778wFmj4nbmb",
        "outputId": "642d7f32-e1dc-41b5-f202-ffc3f0af3210"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Property Type\n",
            "{0: 'Apartment'}\n",
            "Ownership\n",
            "{0: 'By Company', 1: 'By Owner', 2: nan}\n",
            "Payment method\n",
            "{0: 'Cash', 1: 'Cheque', 2: 'Installments', 3: 'Other', 4: nan}\n",
            "Condition\n",
            "{0: 'Ready to move in', 1: 'Under Construction', 2: nan}\n",
            "LULC\n",
            "{0: 'Banana', 1: 'Bare Rocks', 2: 'Clear Grasslands', 3: 'Clear Mixed Wooded Lands', 4: 'Clear Oaks', 5: 'Clear Pines', 6: 'Dense Informal Urban Fabric', 7: 'Dense Mixed Wooded Lands', 8: 'Dense Oaks', 9: 'Dense Pines', 10: 'Dense Urban Fabric', 11: 'Diverse Equipment', 12: 'Fruit Trees', 13: 'Green Sports Area', 14: 'Highway', 15: 'Industrial or Commercial Areas', 16: 'Low Density Urban Fabric', 17: 'Medium Density Urban Fabric', 18: 'Meium Denisty Informal Urban Fabric', 19: 'Olives', 20: 'Port Areas', 21: 'Port Basin', 22: 'Protected Agriculture', 23: 'Rocky Outcrops', 24: 'Scrubland', 25: 'Scrubland with Some Dispersed Bigger Trees', 26: 'Uran Extension and/or Construction Sites', 27: 'Urban Vacant Land'}\n",
            "governorate\n",
            "{0: 'Beyrouth', 1: 'Mont-Liban'}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Find categorical columns\n",
        "categorical_columns = data.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "# Initialize OneHotEncoder\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the data\n",
        "for col in categorical_columns:\n",
        "    data[col] = encoder.fit_transform(data[col])\n",
        "    print(col)\n",
        "    print(dict(enumerate(encoder.classes_)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RpC0frKPOFK"
      },
      "source": [
        "Filling null values using iterative imputer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQ-tTxwAzwHb",
        "outputId": "649d7953-f0ca-4e3e-87a8-f9764e8f460f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7769 entries, 0 to 7768\n",
            "Data columns (total 29 columns):\n",
            " #   Column                    Non-Null Count  Dtype  \n",
            "---  ------                    --------------  -----  \n",
            " 0   Property Type             7769 non-null   int64  \n",
            " 1   Ownership                 7769 non-null   int64  \n",
            " 2   Bedrooms                  7769 non-null   int64  \n",
            " 3   Bathrooms                 7769 non-null   int64  \n",
            " 4   Size (mÂ²)                 7769 non-null   float64\n",
            " 5   Payment method            7769 non-null   int64  \n",
            " 6   Condition                 7769 non-null   int64  \n",
            " 7   Near Amenities            7769 non-null   float64\n",
            " 8   Heating and Cooling       7769 non-null   float64\n",
            " 9   Outdoor and Landscaping   7769 non-null   float64\n",
            " 10  Security Features         7769 non-null   float64\n",
            " 11  Storage and Space         7769 non-null   float64\n",
            " 12  Views                     7769 non-null   float64\n",
            " 13  Technology and Utilities  7769 non-null   float64\n",
            " 14  Luxury and Convenience    7769 non-null   float64\n",
            " 15  Fitness and Recreation    7769 non-null   float64\n",
            " 16  Pet-Friendly              7769 non-null   float64\n",
            " 17  Child-Friendly            7769 non-null   float64\n",
            " 18  LULC                      7769 non-null   int64  \n",
            " 19  governorate               7769 non-null   float64\n",
            " 20  NEAR_PRV_SCH_DIST         7769 non-null   float64\n",
            " 21  NEAR_PUB_SCH_DIST         7769 non-null   float64\n",
            " 22  NEAR_HOSPITAL_DIST        7769 non-null   float64\n",
            " 23  NEAR_NURSERY_DIST         7769 non-null   float64\n",
            " 24  NEAR_PHARMACY_DIST        7769 non-null   float64\n",
            " 25  NEAR_SCHOOL_DIST          7769 non-null   float64\n",
            " 26  DEM                       7769 non-null   float64\n",
            " 27  Population                7769 non-null   float64\n",
            " 28  Price                     7769 non-null   float64\n",
            "dtypes: float64(22), int64(7)\n",
            "memory usage: 1.7 MB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.experimental import enable_iterative_imputer  # Required for IterativeImputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "# Identify column types\n",
        "numerical_features = data.select_dtypes(include=[np.number]).columns\n",
        "binary_features = [col for col in numerical_features if data[col].nunique() == 2]\n",
        "integer_features = [col for col in numerical_features if col not in binary_features and pd.api.types.is_integer_dtype(data[col])]\n",
        "\n",
        "# Apply Iterative Imputer\n",
        "iterative_imputer = IterativeImputer()\n",
        "df_imputed = iterative_imputer.fit_transform(data)\n",
        "\n",
        "# Convert the imputed data back to a DataFrame\n",
        "data = pd.DataFrame(df_imputed, columns=data.columns)\n",
        "\n",
        "# Convert binary features back to 0/1\n",
        "data[binary_features] = data[binary_features].round().clip(0, 1)\n",
        "\n",
        "# Convert integer features back to integers\n",
        "data[integer_features] = data[integer_features].round().astype(int)\n",
        "\n",
        "print(data.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPqJKtsUPYB_"
      },
      "source": [
        "Removing outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "us_809M4buB7",
        "outputId": "4782c0a1-31a9-4c5a-dad4-07222052f6f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6603, 29)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "def remove_outliers_isolation_forest(data):#0.15\n",
        "    iso_forest = IsolationForest(contamination=0.15, random_state=42)\n",
        "    outliers = iso_forest.fit_predict(data.select_dtypes(include=np.number))\n",
        "    # print(data[outliers == 1])\n",
        "    return data[outliers == 1]  # Keep only inliers\n",
        "\n",
        "data = remove_outliers_isolation_forest(data)\n",
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjda9op_Pl9t"
      },
      "source": [
        "Select the most important features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "j-8IN7zCYXOC"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
        "# from sklearn.ensemble import RandomForestRegressor\n",
        "# from sklearn.linear_model import Lasso\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.feature_selection import RFE\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "# import shap\n",
        "# from xgboost import XGBRegressor\n",
        "\n",
        "# # Assuming you have loaded your data into 'data'\n",
        "# X = data.drop(columns=['Price'])\n",
        "# y = data['Price']\n",
        "\n",
        "# # Standardize the data for methods that require scaling\n",
        "# scaler = StandardScaler()\n",
        "# X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# # Dictionary to store feature importance scores\n",
        "# feature_scores = pd.DataFrame(index=X.columns)\n",
        "\n",
        "# # 1. SelectKBest (f_regression)\n",
        "# skb = SelectKBest(score_func=f_regression, k='all')\n",
        "# skb.fit(X, y)\n",
        "# feature_scores['SelectKBest'] = skb.scores_\n",
        "\n",
        "# # 2. Recursive Feature Elimination (RFE) with Linear Regression\n",
        "# rfe = RFE(estimator=LinearRegression(), n_features_to_select=20)\n",
        "# rfe.fit(X, y)\n",
        "# # Since RFE provides rankings, we convert them to importance scores (inverse of ranking)\n",
        "# feature_scores['RFE'] = 1 / rfe.ranking_\n",
        "# print(feature_scores['RFE'])\n",
        "\n",
        "# # 3. Random Forest Feature Importance\n",
        "# rf = RandomForestRegressor()\n",
        "# rf.fit(X, y)\n",
        "# feature_scores['RandomForest'] = rf.feature_importances_\n",
        "\n",
        "# # 4. Lasso Regression\n",
        "# lasso = Lasso(alpha=0.01)\n",
        "# lasso.fit(X_scaled, y)\n",
        "# feature_scores['Lasso'] = np.abs(lasso.coef_)\n",
        "\n",
        "# # 5. Mutual Information\n",
        "# mi = mutual_info_regression(X, y)\n",
        "# feature_scores['MutualInfo'] = mi\n",
        "\n",
        "# # 6. SHAP Values with XGBoost\n",
        "# xgb = XGBRegressor()\n",
        "# xgb.fit(X, y)\n",
        "# explainer = shap.Explainer(xgb)\n",
        "# shap_values = explainer(X)\n",
        "# shap_importance = np.abs(shap_values.values).mean(axis=0)\n",
        "# feature_scores['SHAP'] = shap_importance\n",
        "\n",
        "# # Normalize scores for better comparison (0 to 1)\n",
        "# feature_scores = feature_scores.apply(lambda x: (x - np.min(x)) / (np.max(x) - np.min(x)))\n",
        "\n",
        "# # Select top 20 features based on the average score across all methods\n",
        "# top_features = feature_scores.mean(axis=1).nlargest(20).index\n",
        "\n",
        "# # Filter scores for top features only\n",
        "# top_feature_scores = feature_scores.loc[top_features]\n",
        "\n",
        "# # Plot the grouped bar chart\n",
        "# top_feature_scores.plot(kind='bar', figsize=(15, 8), width=0.8)\n",
        "# plt.title('Feature Importance Scores Across Different Methods')\n",
        "# plt.xlabel('Features')\n",
        "# plt.ylabel('Normalized Importance Score')\n",
        "# plt.xticks(rotation=45, ha='right')\n",
        "# plt.legend(title='Methods')\n",
        "# plt.grid(axis='y')\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "RD3E4if2HSff"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.feature_selection import mutual_info_regression\n",
        "\n",
        "# # Assuming you have loaded your data into 'data'\n",
        "# X = data.drop(columns=['Price'])\n",
        "# y = data['Price']\n",
        "\n",
        "# # Calculate mutual information\n",
        "# mi_scores = mutual_info_regression(X, y)\n",
        "# mi_scores = pd.Series(mi_scores, index=X.columns)\n",
        "\n",
        "# # Select top 20 features\n",
        "# selected_features = mi_scores.nlargest(25)\n",
        "# print(selected_features)\n",
        "\n",
        "# # Plot the mutual information scores for the top 20 features\n",
        "# plt.figure(figsize=(12, 8))\n",
        "# bars = plt.barh(selected_features.index, selected_features.values, color='skyblue')\n",
        "# plt.xlabel('Mutual Information Score')\n",
        "# plt.title('Top 20 Features Based on Mutual Information')\n",
        "\n",
        "# # Annotate bars with the score values\n",
        "# for bar in bars:\n",
        "#     plt.text(bar.get_width() + 0.001, bar.get_y() + bar.get_height()/2,\n",
        "#              f'{bar.get_width():.3f}', va='center', ha='left')\n",
        "\n",
        "# plt.gca().invert_yaxis()  # Invert y-axis to have the highest score at the top\n",
        "# plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# X = data[selected_features.index.tolist()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jIinwFW38_D"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "data.to_excel('/content/drive/Othercomputers/MyMonty Laptop/university M2/M2 final project/Dataset/step 3 - with spatial analysis/FINAL_SELL_REAL_ESTATE_2024-11-17-2.xlsx', index=False)\n",
        "\n",
        "# Step 5: Calculate the correlation or other metric\n",
        "X = data.drop(columns=['Price'])\n",
        "y = data['Price']\n",
        "\n",
        "# Perform feature selection using f_regression\n",
        "selector = SelectKBest(score_func=f_regression, k='all')\n",
        "selector.fit(X, y)\n",
        "\n",
        "# Create a DataFrame for feature scores\n",
        "feature_scores = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Score': selector.scores_\n",
        "}).sort_values(by='Score', ascending=False)  # Sort by score for easy ranking\n",
        "\n",
        "# Step 6: Select the most important 24 features\n",
        "selected_features = feature_scores.nlargest(14, 'Score')\n",
        "print(\"Top 24 Selected Features:\")\n",
        "print(selected_features['Feature'].tolist())\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "bars = plt.barh(selected_features['Feature'], selected_features['Score'], color='skyblue')\n",
        "plt.xlabel('Score (F-Regression)')\n",
        "plt.title('Top 20 Features Based on F-Regression Scores')\n",
        "\n",
        "# Annotate bars with the score values\n",
        "for bar in bars:\n",
        "    plt.text(\n",
        "        bar.get_width() + 0.1, bar.get_y() + bar.get_height() / 2,\n",
        "        f'{bar.get_width():.3f}', va='center', ha='left'\n",
        "    )\n",
        "\n",
        "# Invert y-axis to display the highest score at the top\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Update X to include only the selected features\n",
        "X = X[selected_features['Feature'].tolist()]\n",
        "\n",
        "# Check the shapes of X and y\n",
        "print(\"Feature Matrix Shape:\", X.shape)\n",
        "print(\"Target Variable Shape:\", y.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxdZCHvk416o"
      },
      "source": [
        "## House Price Distribution\n",
        "\n",
        "Now let us take a look at how the house prices are distributed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBg0jkb-TLxw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming your DataFrame is named 'data'\n",
        "for col in X.columns:\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    if pd.api.types.is_numeric_dtype(data[col]):\n",
        "        # Plot histogram for numerical columns\n",
        "        plt.hist(data[col].dropna(), bins=30, color='skyblue', edgecolor='black')\n",
        "        plt.title(f'Histogram of {col}')\n",
        "        plt.xlabel(col)\n",
        "        plt.ylabel('Frequency')\n",
        "    elif pd.api.types.is_categorical_dtype(data[col]) or data[col].dtype == 'object':\n",
        "        # Plot bar plot for categorical columns\n",
        "        sns.countplot(data=data, x=col, palette='viridis')\n",
        "        plt.title(f'Distribution of {col}')\n",
        "        plt.xlabel(col)\n",
        "        plt.ylabel('Count')\n",
        "        plt.xticks(rotation=45)\n",
        "    else:\n",
        "        # Skip columns with unsupported data types\n",
        "        print(f\"Skipping column: {col} (unsupported data type)\")\n",
        "        continue\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qROZWZyE416o"
      },
      "outputs": [],
      "source": [
        "print(data['Price'].describe())\n",
        "plt.figure(figsize=(9, 8))\n",
        "sns.distplot(data['Price'], color='g', bins=100, hist_kws={'alpha': 0.4});"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKnn1nR-416o"
      },
      "source": [
        "## Numerical data distribution\n",
        "\n",
        "We will now take a look at how the numerical features are distributed. In order to do this, let us first list all the types of data from our dataset and select only the numerical ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-OBdLgjnjpo"
      },
      "outputs": [],
      "source": [
        "df_num = X.select_dtypes(include=[np.number])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ow3dWseC5bf"
      },
      "outputs": [],
      "source": [
        "df_num.hist(figsize=(12, 12), bins=50, xlabelsize=6, ylabelsize=6);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jspkCkTNEZEd"
      },
      "outputs": [],
      "source": [
        "# Set up subplots for each feature\n",
        "num_plots = len(df_num)\n",
        "\n",
        "# Plot each feature against the target\n",
        "for feature in selected_features['Feature'].tolist():\n",
        "    plt.figure(figsize=(8, 6))  # Adjust the figure size as needed\n",
        "    sns.scatterplot(x=feature, y=y, data=X)\n",
        "    plt.title(f'{feature} vs House price')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('House Price')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McM8zwZlSppB"
      },
      "outputs": [],
      "source": [
        "# Step 5: Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBQVNCBB2QJu"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define regressors\n",
        "regressors = {\n",
        "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
        "    \"AdaBoost\": AdaBoostRegressor(random_state=42),\n",
        "    \"Linear Regression\": LinearRegression(),\n",
        "    \"Ridge\": Ridge(alpha=1.0, random_state=42),\n",
        "    \"Lasso\": Lasso(alpha=1.0, random_state=42),\n",
        "    \"ElasticNet\": ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42),\n",
        "    \"K-Nearest Neighbors\": KNeighborsRegressor(),\n",
        "    \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n",
        "    \"Extra Trees\": ExtraTreesRegressor(n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "# Initialize a list to store results\n",
        "results = []\n",
        "\n",
        "# Train and evaluate each regressor\n",
        "for name, reg in regressors.items():\n",
        "    # Fit the model\n",
        "    reg.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = reg.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r_squared = reg.score(X_test, y_test)\n",
        "\n",
        "    # Append the results\n",
        "    results.append({\n",
        "        \"Model\": name,\n",
        "        \"MSE\": mse,\n",
        "        \"RMSE\": rmse,\n",
        "        \"MAE\": mae,\n",
        "        \"R-squared\": r_squared\n",
        "    })\n",
        "\n",
        "# Create a DataFrame to display the results\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.sort_values(by=\"RMSE\").reset_index(drop=True)\n",
        "\n",
        "# Display the results\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4esaNzNxb3CG"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import GridSearchCV\n",
        "# from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "# # Define regressors and their parameter grids\n",
        "# param_grids = {\n",
        "#     \"Random Forest\": (RandomForestRegressor(random_state=42), {\n",
        "#         \"n_estimators\": [100, 200, 500],\n",
        "#         \"max_depth\": [None, 10, 20],\n",
        "#         \"min_samples_split\": [2, 5, 10],\n",
        "#         \"min_samples_leaf\": [1, 2, 4]\n",
        "#     }),\n",
        "#     \"Extra Trees\": (ExtraTreesRegressor(random_state=42), {\n",
        "#         \"n_estimators\": [100, 200, 500],\n",
        "#         \"max_depth\": [None, 10, 20],\n",
        "#         \"min_samples_split\": [2, 5, 10]\n",
        "#     }),\n",
        "#     \"Gradient Boosting\": (GradientBoostingRegressor(random_state=42), {\n",
        "#         \"n_estimators\": [100, 200, 500],\n",
        "#         \"learning_rate\": [0.01, 0.1, 0.2],\n",
        "#         \"max_depth\": [3, 5, 7],\n",
        "#         \"min_samples_split\": [2, 5, 10]\n",
        "#     }),\n",
        "#     \"AdaBoost\": (AdaBoostRegressor(random_state=42), {\n",
        "#         \"n_estimators\": [50, 100, 200],\n",
        "#         \"learning_rate\": [0.01, 0.1, 1.0]\n",
        "#     }),\n",
        "#     \"Ridge\": (Ridge(), {\n",
        "#         \"alpha\": [0.001, 0.01, 0.1, 1, 10, 100]\n",
        "#     }),\n",
        "#     \"Lasso\": (Lasso(), {\n",
        "#         \"alpha\": [0.001, 0.01, 0.1, 1, 10, 100]\n",
        "#     }),\n",
        "#     \"ElasticNet\": (ElasticNet(random_state=42), {\n",
        "#         \"alpha\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "#         \"l1_ratio\": [0.1, 0.5, 0.9]\n",
        "#     }),\n",
        "#     \"K-Nearest Neighbors\": (KNeighborsRegressor(), {\n",
        "#         \"n_neighbors\": [3, 5, 10, 20],\n",
        "#         \"weights\": ['uniform', 'distance'],\n",
        "#         \"p\": [1, 2]\n",
        "#     }),\n",
        "#     \"Decision Tree\": (DecisionTreeRegressor(random_state=42), {\n",
        "#         \"max_depth\": [None, 5, 10, 20],\n",
        "#         \"min_samples_split\": [2, 5, 10],\n",
        "#         \"min_samples_leaf\": [1, 2, 4]\n",
        "#     })\n",
        "# }\n",
        "\n",
        "# # Initialize a list to store results\n",
        "# results = []\n",
        "\n",
        "# # Train and evaluate each regressor using GridSearchCV\n",
        "# for name, (reg, param_grid) in param_grids.items():\n",
        "#     print(f\"Running GridSearchCV for {name}...\")\n",
        "#     grid_search = GridSearchCV(estimator=reg, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "#     grid_search.fit(X_train, y_train)\n",
        "\n",
        "#     best_model = grid_search.best_estimator_\n",
        "#     y_pred = best_model.predict(X_test)\n",
        "\n",
        "#     # Calculate metrics\n",
        "#     mse = mean_squared_error(y_test, y_pred)\n",
        "#     rmse = np.sqrt(mse)\n",
        "#     mae = mean_absolute_error(y_test, y_pred)\n",
        "#     r_squared = best_model.score(X_test, y_test)\n",
        "\n",
        "#     # Display the result for the current model\n",
        "#     print(f\"\\nModel: {name}\")\n",
        "#     print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "#     print(f\"MSE: {mse:.4f}\")\n",
        "#     print(f\"RMSE: {rmse:.4f}\")\n",
        "#     print(f\"MAE: {mae:.4f}\")\n",
        "#     print(f\"R-squared: {r_squared:.4f}\\n\")\n",
        "\n",
        "#     # Append the results to the list\n",
        "#     results.append({\n",
        "#         \"Model\": name,\n",
        "#         \"Best Parameters\": grid_search.best_params_,\n",
        "#         \"MSE\": mse,\n",
        "#         \"RMSE\": rmse,\n",
        "#         \"MAE\": mae,\n",
        "#         \"R-squared\": r_squared\n",
        "#     })\n",
        "\n",
        "# # Create a DataFrame to display the results\n",
        "# results_df = pd.DataFrame(results)\n",
        "# results_df = results_df.sort_values(by=\"RMSE\").reset_index(drop=True)\n",
        "\n",
        "# # Display the final sorted results\n",
        "# print(\"\\nModel Performance Results:\")\n",
        "# print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "fuHoStStoW5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import VotingRegressor, StackingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into training and test sets (example setup)\n",
        "# Replace X and y with your actual data\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the objective function for each model\n",
        "def objective(trial, model_name, return_model=False):\n",
        "    if model_name == \"Random Forest\":\n",
        "        from sklearn.ensemble import RandomForestRegressor\n",
        "        reg = RandomForestRegressor(\n",
        "            n_estimators=trial.suggest_categorical(\"n_estimators\", [100, 200, 500]),\n",
        "            max_depth=trial.suggest_categorical(\"max_depth\", [None, 10, 20]),\n",
        "            min_samples_split=trial.suggest_categorical(\"min_samples_split\", [2, 5, 10]),\n",
        "            min_samples_leaf=trial.suggest_categorical(\"min_samples_leaf\", [1, 2, 4]),\n",
        "            random_state=42\n",
        "        )\n",
        "    elif model_name == \"Extra Trees\":\n",
        "        from sklearn.ensemble import ExtraTreesRegressor\n",
        "        reg = ExtraTreesRegressor(\n",
        "            n_estimators=trial.suggest_categorical(\"n_estimators\", [100, 200, 500]),\n",
        "            max_depth=trial.suggest_categorical(\"max_depth\", [None, 10, 20]),\n",
        "            min_samples_split=trial.suggest_categorical(\"min_samples_split\", [2, 5, 10]),\n",
        "            random_state=42\n",
        "        )\n",
        "    elif model_name == \"Gradient Boosting\":\n",
        "        from sklearn.ensemble import GradientBoostingRegressor\n",
        "        reg = GradientBoostingRegressor(\n",
        "            n_estimators=trial.suggest_categorical(\"n_estimators\", [100, 200, 500]),\n",
        "            learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
        "            max_depth=trial.suggest_categorical(\"max_depth\", [3, 5, 7]),\n",
        "            min_samples_split=trial.suggest_categorical(\"min_samples_split\", [2, 5, 10]),\n",
        "            random_state=42\n",
        "        )\n",
        "    elif model_name == \"AdaBoost\":\n",
        "        from sklearn.ensemble import AdaBoostRegressor\n",
        "        reg = AdaBoostRegressor(\n",
        "            n_estimators=trial.suggest_categorical(\"n_estimators\", [50, 100, 200]),\n",
        "            learning_rate=trial.suggest_float(\"learning_rate\", 0.01, 1.0),\n",
        "            random_state=42\n",
        "        )\n",
        "    elif model_name == \"Ridge\":\n",
        "        from sklearn.linear_model import Ridge\n",
        "        reg = Ridge(\n",
        "            alpha=trial.suggest_loguniform(\"alpha\", 0.001, 100)\n",
        "        )\n",
        "    elif model_name == \"Lasso\":\n",
        "        from sklearn.linear_model import Lasso\n",
        "        reg = Lasso(\n",
        "            alpha=trial.suggest_loguniform(\"alpha\", 0.001, 100)\n",
        "        )\n",
        "    elif model_name == \"ElasticNet\":\n",
        "        from sklearn.linear_model import ElasticNet\n",
        "        reg = ElasticNet(\n",
        "            alpha=trial.suggest_loguniform(\"alpha\", 0.001, 100),\n",
        "            l1_ratio=trial.suggest_float(\"l1_ratio\", 0.1, 0.9),\n",
        "            random_state=42\n",
        "        )\n",
        "    elif model_name == \"K-Nearest Neighbors\":\n",
        "        from sklearn.neighbors import KNeighborsRegressor\n",
        "        reg = KNeighborsRegressor(\n",
        "            n_neighbors=trial.suggest_categorical(\"n_neighbors\", [3, 5, 10, 20]),\n",
        "            weights=trial.suggest_categorical(\"weights\", ['uniform', 'distance']),\n",
        "            p=trial.suggest_categorical(\"p\", [1, 2])\n",
        "        )\n",
        "    elif model_name == \"Decision Tree\":\n",
        "        from sklearn.tree import DecisionTreeRegressor\n",
        "        reg = DecisionTreeRegressor(\n",
        "            max_depth=trial.suggest_categorical(\"max_depth\", [None, 5, 10, 20]),\n",
        "            min_samples_split=trial.suggest_categorical(\"min_samples_split\", [2, 5, 10]),\n",
        "            min_samples_leaf=trial.suggest_categorical(\"min_samples_leaf\", [1, 2, 4]),\n",
        "            random_state=42\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
        "\n",
        "\n",
        "    reg.fit(X_train, y_train)\n",
        "\n",
        "    if return_model:\n",
        "        return reg  # Return the trained model\n",
        "\n",
        "    y_pred = reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    return mse  # Optuna minimizes the objective\n",
        "\n",
        "# Models to optimize\n",
        "models = [\"Random Forest\", \"Extra Trees\", \"Gradient Boosting\", \"AdaBoost\",\n",
        "          \"Ridge\", \"Lasso\", \"ElasticNet\", \"K-Nearest Neighbors\", \"Decision Tree\"]\n",
        "\n",
        "# Results storage\n",
        "results = []\n",
        "\n",
        "# Run Optuna for each model\n",
        "for model_name in models:\n",
        "    print(f\"Optimizing {model_name}...\")\n",
        "\n",
        "    study = optuna.create_study(direction=\"minimize\")\n",
        "    study.optimize(lambda trial: objective(trial, model_name), n_trials=50)\n",
        "\n",
        "    best_params = study.best_params\n",
        "    best_value = study.best_value\n",
        "    print(f\"Best parameters for {model_name}: {best_params}\")\n",
        "    print(f\"Best MSE: {best_value:.4f}\")\n",
        "\n",
        "    # Re-train the model with the best parameters\n",
        "    trained_model = objective(study.best_trial, model_name, return_model=True)\n",
        "    y_pred = trained_model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r_squared = trained_model.score(X_test, y_test)\n",
        "\n",
        "    results.append({\n",
        "        \"Model\": model_name,\n",
        "        \"MSE\": mse,\n",
        "        \"RMSE\": rmse,\n",
        "        \"MAE\": mae,\n",
        "        \"R-squared\": r_squared,\n",
        "        \"Best Parameters\": best_params\n",
        "    })\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.sort_values(by=\"RMSE\").reset_index(drop=True)\n",
        "\n",
        "# Display results\n",
        "print(\"\\nModel Performance Results:\")\n",
        "print(results_df)"
      ],
      "metadata": {
        "id": "uxRsvJm28-CJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import VotingRegressor, StackingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.base import clone\n",
        "\n",
        "# Step 1: Prepare base models for ensemble methods using the best parameters\n",
        "model_instances = []\n",
        "for res in results:\n",
        "    model_name = res[\"Model\"]\n",
        "    best_params = res[\"Best Parameters\"]\n",
        "\n",
        "    if model_name == \"Random Forest\":\n",
        "        from sklearn.ensemble import RandomForestRegressor\n",
        "        model = RandomForestRegressor(random_state=42, **best_params)\n",
        "    elif model_name == \"Extra Trees\":\n",
        "        from sklearn.ensemble import ExtraTreesRegressor\n",
        "        model = ExtraTreesRegressor(random_state=42, **best_params)\n",
        "    elif model_name == \"Gradient Boosting\":\n",
        "        from sklearn.ensemble import GradientBoostingRegressor\n",
        "        model = GradientBoostingRegressor(random_state=42, **best_params)\n",
        "    elif model_name == \"AdaBoost\":\n",
        "        from sklearn.ensemble import AdaBoostRegressor\n",
        "        model = AdaBoostRegressor(random_state=42, **best_params)\n",
        "    elif model_name == \"Ridge\":\n",
        "        from sklearn.linear_model import Ridge\n",
        "        model = Ridge(**best_params)\n",
        "    elif model_name == \"Lasso\":\n",
        "        from sklearn.linear_model import Lasso\n",
        "        model = Lasso(**best_params)\n",
        "    elif model_name == \"ElasticNet\":\n",
        "        from sklearn.linear_model import ElasticNet\n",
        "        model = ElasticNet(random_state=42, **best_params)\n",
        "    elif model_name == \"K-Nearest Neighbors\":\n",
        "        from sklearn.neighbors import KNeighborsRegressor\n",
        "        model = KNeighborsRegressor(**best_params)\n",
        "    elif model_name == \"Decision Tree\":\n",
        "        from sklearn.tree import DecisionTreeRegressor\n",
        "        model = DecisionTreeRegressor(random_state=42, **best_params)\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
        "\n",
        "    model_instances.append((model_name, clone(model)))\n",
        "\n",
        "# Step 2: Define Voting Regressor\n",
        "voting_regressor = VotingRegressor(estimators=model_instances)\n",
        "\n",
        "# Train Voting Regressor\n",
        "voting_regressor.fit(X_train, y_train)\n",
        "y_pred_voting = voting_regressor.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "mse_voting = mean_squared_error(y_test, y_pred_voting)\n",
        "rmse_voting = np.sqrt(mse_voting)\n",
        "mae_voting = mean_absolute_error(y_test, y_pred_voting)\n",
        "r_squared_voting = voting_regressor.score(X_test, y_test)\n",
        "\n",
        "results.append({\n",
        "    \"Model\": \"Voting Regressor\",\n",
        "    \"MSE\": mse_voting,\n",
        "    \"RMSE\": rmse_voting,\n",
        "    \"MAE\": mae_voting,\n",
        "    \"R-squared\": r_squared_voting,\n",
        "    \"Best Parameters\": \"Combined\"\n",
        "})\n",
        "\n",
        "# Step 3: Define Stacking Regressor\n",
        "# Using Linear Regression as the final estimator\n",
        "final_estimator = LinearRegression()\n",
        "stacking_regressor = StackingRegressor(estimators=model_instances, final_estimator=final_estimator)\n",
        "\n",
        "# Train Stacking Regressor\n",
        "stacking_regressor.fit(X_train, y_train)\n",
        "y_pred_stacking = stacking_regressor.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "mse_stacking = mean_squared_error(y_test, y_pred_stacking)\n",
        "rmse_stacking = np.sqrt(mse_stacking)\n",
        "mae_stacking = mean_absolute_error(y_test, y_pred_stacking)\n",
        "r_squared_stacking = stacking_regressor.score(X_test, y_test)\n",
        "\n",
        "results.append({\n",
        "    \"Model\": \"Stacking Regressor\",\n",
        "    \"MSE\": mse_stacking,\n",
        "    \"RMSE\": rmse_stacking,\n",
        "    \"MAE\": mae_stacking,\n",
        "    \"R-squared\": r_squared_stacking,\n",
        "    \"Best Parameters\": \"Combined\"\n",
        "})\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.sort_values(by=\"RMSE\").reset_index(drop=True)\n",
        "\n",
        "# Display results\n",
        "print(\"\\nModel Performance Results:\")\n",
        "print(results_df)"
      ],
      "metadata": {
        "id": "tZXDc7hkN24z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDj-M7apS_ZD"
      },
      "outputs": [],
      "source": [
        "# Create a Gradient Boosting Regressor model\n",
        "# gb_model = ExtraTreesRegressor(\n",
        "#     max_depth=20,\n",
        "#     min_samples_split=5,\n",
        "#     n_estimators=500,\n",
        "#     random_state=42,  # For reproducibility\n",
        "#     n_jobs=-1         # Utilize all CPU cores for training\n",
        "# )\n",
        "gb_model = stacking_regressor\n",
        "# gb_model = KNeighborsRegressor(\n",
        "#     n_neighbors=5,  # Number of neighbors\n",
        "#     p=2,            # Minkowski distance metric (p=2 corresponds to Euclidean distance)\n",
        "#     weights='distance'  # Weight by distance (closer neighbors have higher influence)\n",
        "# )\n",
        "\n",
        "# Best Parameters: {'learning_rate': 0.1, 'max_depth': 7, 'min_samples_split': 10, 'n_estimators': 100}\n",
        "# Train the model on the training data\n",
        "gb_model.fit(X_train, y_train)\n",
        "y_pred = gb_model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Calculate R-squared\n",
        "r_squared = gb_model.score(X_test, y_test)\n",
        "\n",
        "# print(name)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r_squared)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9COAtaJWi41"
      },
      "outputs": [],
      "source": [
        "# Create a scatter plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, y_pred, color='blue', alpha=0.5)\n",
        "\n",
        "# Add labels and title\n",
        "plt.title('Actual vs. Predicted Values')\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "\n",
        "# Add a diagonal line representing perfect predictions\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1HJ6KxRT7IR"
      },
      "source": [
        "## Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxissJBtrsi4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgdd6WOAPVj-"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "import joblib\n",
        "print(sklearn.__version__)\n",
        "print(joblib.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aak3WafNsoOq"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "# Define the path in your Google Drive where you want to save the model\n",
        "model_path = \"/content/drive/MyDrive/university M2/M2 final project/finalModel/\"\n",
        "\n",
        "# Save the model to a file\n",
        "joblib.dump(gb_model, f'{model_path}StackingRegressor2.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYMXEhd7H0JW"
      },
      "outputs": [],
      "source": [
        "# Load the model from the file\n",
        "loaded_model = joblib.load(f'{model_path}ExtraTreesRegressor.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuLlmFVlnrLs"
      },
      "outputs": [],
      "source": [
        "print(X_valid.iloc[22])\n",
        "print(y_valid.iloc[22])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVX9sUh1nmcX"
      },
      "outputs": [],
      "source": [
        "# y_pred = loaded_model.predict(X_valid.iloc[[22]])\n",
        "y_pred = loaded_model.predict(X_valid)\n",
        "print(y_pred)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_valid, y_pred)\n",
        "r2 = r2_score(y_valid, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R-squared: {r2}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "databundleVersionId": 868283,
          "sourceId": 5407,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30407,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}